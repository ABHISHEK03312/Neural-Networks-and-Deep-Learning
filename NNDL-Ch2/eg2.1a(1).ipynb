{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2.1: SGD of a linear neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import os\n",
    "if not os.path.isdir('figures'):\n",
    "\tos.makedirs('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "no_epochs = 200\n",
    "lr = 0.01\n",
    "\n",
    "SEED = 10\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "X = 2*np.random.rand(6, 2) - 1\n",
    "Y = np.dot(X, [2.53, -0.47]) + np.random.rand(6) - 0.5\n",
    "\n",
    "print('X: {}'.format(X))\n",
    "print('Y: {}'.format(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for a linear neuron\n",
    "class Linear():\n",
    "  def __init__(self):\n",
    "    self.w = tf.Variable(np.random.rand(2), dtype=tf.float64)\n",
    "    self.b = tf.Variable(0., dtype=tf.float64)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return tf.tensordot(x ,self.w, axes=1) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squared error as the loss function\n",
    "def loss(predicted_y, target_y):\n",
    "  return tf.square(predicted_y - target_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion executing a training step\n",
    "def train_step(model, x, d, learning_rate):\n",
    "    y = model(x)\n",
    "    grad_w = -(d - y)*x\n",
    "    grad_b = -(d - y)\n",
    "    model.w.assign(model.w - learning_rate * grad_w)\n",
    "    model.b.assign(model.b - learning_rate * grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function to train the neuron starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear()\n",
    "print('w: {}, b: {}'.format(model.w.numpy(), model.b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep an index for training\n",
    "idx = np.arange(len(X))\n",
    "\n",
    "err = []\n",
    "for epoch in range(no_epochs):\n",
    "  np.random.shuffle(idx)\n",
    "  X, Y = X[idx], Y[idx]\n",
    "    \n",
    "  err_ = []\n",
    "  for p in np.arange(len(X)):\n",
    "    \n",
    "    y_ = model(X[p])\n",
    "    loss_ = loss(y_, Y[p])\n",
    "    \n",
    "    train_step(model, X[p], Y[p], learning_rate=lr)\n",
    "    \n",
    "    err_.append(loss_)\n",
    "    \n",
    "    if epoch == 0:\n",
    "      print('iter: {}'.format(epoch+1))\n",
    "      print('p: {}'.format(p+1))\n",
    "      print('x:{}, d:{}'.format(X[p], Y[p]))\n",
    "      print('y: {}'.format(y_))\n",
    "      print('se: {}'.format(loss_))\n",
    "      print('w: {}, b: {}'.format(model.w.numpy(), model.b.numpy()))\n",
    " \n",
    "  err.append(np.mean(err_))\n",
    "  if epoch%10 == 0:\n",
    "        print('iter: %3d, mse: %1.4f'%(epoch, err[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print learned weights\n",
    "print('w: %s, b: %s'%(model.w.numpy(), model.b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print learning curve\n",
    "plt.figure(1)\n",
    "plt.plot(range(no_epochs), err)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse')\n",
    "plt.savefig('./figures/2.1a_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for p in np.arange(len(X)):\n",
    "\tpred.append(model(X[p]).numpy())\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print targets and predictions\n",
    "fig = plt.figure(2)\n",
    "ax = fig.gca(projection = '3d')\n",
    "plot_original = ax.scatter(X[:,0], X[:,1], Y, c='blue', label='targets')\n",
    "plot_pred = ax.scatter(X[:,0], X[:,1], pred, c='orange', label='predicted')\n",
    "X1 = np.arange(-1, 1, 0.1)\n",
    "X2 = np.arange(-1, 1, 0.1)\n",
    "X1,X2 = np.meshgrid(X1,X2)\n",
    "Z = model.w.numpy()[0]*X1 + model.w.numpy()[1]*X2 + model.b.numpy()\n",
    "regression_plane = ax.plot_surface(X1, X2, Z)\n",
    "ax.set_zticks([ -2, -1, 0, 1])\n",
    "ax.set_xticks([-0.5, 0, 0.5])\n",
    "ax.set_yticks([-0.5, 0, 0.5])\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$y$')\n",
    "plt.legend()\n",
    "plt.savefig('./figures/2.1a_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
